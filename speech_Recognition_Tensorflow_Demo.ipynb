{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speech Recognition Tensorflow Demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desai-nitin/BootstrapPortfolioProject/blob/master/speech_Recognition_Tensorflow_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEQggY9ryS6Y",
        "colab_type": "text"
      },
      "source": [
        "# Speech recognition using a CNN\n",
        "\n",
        "This is a simple example notebook for a speech recognition task using a 2D Convolutional Neural Network. This architecture may not provide the state of art results, but it is easy to understand, fast and lightweight (244.2K parameters and 9.7M calculations). The same CNN model could be useful for pattern recognition in 1D signals other than sound as vibration signals, earthquake signals, especially when the training dataset size is small.\n",
        "\n",
        "# Notebook Index\n",
        "- [Architecture overview](#overview)\n",
        "- [Example dataset](#dataset)\n",
        "- [Data preprocessing](#preprocessing)\n",
        "- [Model](#model)\n",
        "- [Training and evaluation](#training)\n",
        "- [Predictions for streaming audio](#serving)\n",
        "- [References](#references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENpwoWeayo3S",
        "colab_type": "text"
      },
      "source": [
        "## Architecture overview<a id='overview'></a>\n",
        "\n",
        "To train a speech recognition model, we need audio recordings of speech and the corresponding word labels. The audio recording is a single or multi-channel sequence of amplitude values recorded on equal intervals over time ([sample rate](https://en.wikipedia.org/wiki/Sampling_(signal_processing))).\n",
        "\n",
        "Sequence to Sequence models are trained on in multi-word audio and inference multi-word predictions. They can produce better results but will be more computationally expensive. This implementation uses a fixed window of audio recording as input. The window length is set to be able to fit a single word. The training dataset is a collection of individual word recordings and corresponding labels. To process streaming audio, we use overlapping moving window technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haNQxWwhyuL6",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPbE9FxCzGWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install matplotlib numpy scipy scikit-learn pandas tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS5GwnX4yGtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "from tensorflow.contrib.framework.python.ops import audio_ops\n",
        "from scipy.io import wavfile\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5oDvtfay5GK",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87xOTcu1y6KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "\n",
        "data_url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "data_dir = 'speech_dataset'\n",
        "model_dir = 'model_dir'\n",
        "vocabulary = ['__other__', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "test_size = .1\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "max_training_steps = 300000\n",
        "sample_rate = 16000\n",
        "window_size_ms = 30.\n",
        "window_stride_ms = 10.\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.WARN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC9FtBULy_5a",
        "colab_type": "text"
      },
      "source": [
        "## Example dataset<a id='dataset'></a>\n",
        "\n",
        "This example uses the [speech commands dataset v0.02](https://arxiv.org/pdf/1804.03209.pdf). It is a collection of 105,000 single-word [WAVE](https://en.wikipedia.org/wiki/WAV) audio files in folders that represent the label. You can easily record your dataset that follows this folder convention (recordings of the word \"cat\" go inside a folder named \"cat\"). If the recordings are in a format different than WAVE, then you will have to modify the decoding function.\n",
        "\n",
        "### Download and extract the speech commands dataset v0.02\n",
        "\n",
        "The archive file is about 2GB and will be downloaded and extracted the first time you run next code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Emk_0gtzBZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "37ac693b-29a1-4730-8f50-115b735c2e97"
      },
      "source": [
        "# download maybe\n",
        "if not os.path.exists(data_dir):\n",
        "    !wget $data_url -P $data_dir\n",
        "    print('Extracting ...')\n",
        "    !tar -xzf {data_dir}/speech_commands_v0.02.tar.gz -C $data_dir\n",
        "\n",
        "display(ipd.Audio(os.path.join(data_dir, 'happy/0227998e_nohash_1.wav')))\n",
        "!ls speech_dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-02 14:48:36--  http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.69.128, 2607:f8b0:4001:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.69.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428923189 (2.3G) [application/gzip]\n",
            "Saving to: ‘speech_dataset/speech_commands_v0.02.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   2.26G  21.5MB/s    in 43s     \n",
            "\n",
            "2019-10-02 14:49:19 (54.1 MB/s) - ‘speech_dataset/speech_commands_v0.02.tar.gz’ saved [2428923189/2428923189]\n",
            "\n",
            "Extracting ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "_background_noise_  four     on\t\t\t\t   tree\n",
            "backward\t    go\t     one\t\t\t   two\n",
            "bed\t\t    happy    README.md\t\t\t   up\n",
            "bird\t\t    house    right\t\t\t   validation_list.txt\n",
            "cat\t\t    learn    seven\t\t\t   visual\n",
            "dog\t\t    left     sheila\t\t\t   wow\n",
            "down\t\t    LICENSE  six\t\t\t   yes\n",
            "eight\t\t    marvin   speech_commands_v0.02.tar.gz  zero\n",
            "five\t\t    nine     stop\n",
            "follow\t\t    no\t     testing_list.txt\n",
            "forward\t\t    off      three\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07TYEolAzj9A",
        "colab_type": "text"
      },
      "source": [
        "### Stratified train-evaluation split\n",
        "\n",
        "This implementation is limited by the vocabulary of eight words + one special class labeled \"\\_\\_other\\_\\_\". The \"\\_\\_other\\_\\_\" class is a random sample of other words that are not in the vocabulary. An integer ID represents every word according to its position in the vocabulary list. The only input feature for the model is the WAVE filename."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtcmX9VUzuP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0d0c9c66-1521-43a2-a3cf-7c3a939bc541"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "word_files = []\n",
        "word_ids = []\n",
        "for word_id, word in enumerate(vocabulary):\n",
        "    if word == '__other__':\n",
        "        continue\n",
        "    word_dir = os.path.join(data_dir, word)\n",
        "    files = [os.path.join(data_dir, word, f) for f in os.listdir(word_dir)]\n",
        "    assert len(files)\n",
        "    word_files += files\n",
        "    word_ids += [word_id] * len(files)\n",
        "\n",
        "if '__other__' in vocabulary:\n",
        "    all_other_dirs = [i for i in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, i))]\n",
        "    all_other_dirs = [i for i in all_other_dirs if i not in vocabulary]\n",
        "    other_files = []\n",
        "    for word_dir in all_other_dirs:\n",
        "        other_files += [os.path.join(data_dir, word_dir, f) for f in os.listdir(os.path.join(data_dir, word_dir))]\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(other_files)\n",
        "    average_examples_per_word = len(word_ids) // (len(vocabulary)-1)\n",
        "    other_count = average_examples_per_word * 2\n",
        "    word_files += other_files[:other_count]\n",
        "    word_ids += [vocabulary.index('__other__')] * other_count\n",
        "\n",
        "word_files = np.array(word_files)\n",
        "word_ids = np.array(word_ids)\n",
        "\n",
        "train_inputs, eval_inputs, train_labels, eval_labels = train_test_split(\n",
        "    word_files, word_ids, test_size=test_size, stratify=word_ids, random_state=0)\n",
        "\n",
        "print('Train size: {}, Evaluation size:{}'.format(len(train_inputs), len(eval_inputs)))\n",
        "print('*' * 50)\n",
        "print('Sample of evaluation inputs:')\n",
        "print(eval_inputs[:5])\n",
        "print('*' * 50)\n",
        "print('Sample of evaluation labels:')\n",
        "print(eval_labels[:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 34380, Evaluation size:3821\n",
            "**************************************************\n",
            "Sample of evaluation inputs:\n",
            "['speech_dataset/up/879a2b38_nohash_0.wav'\n",
            " 'speech_dataset/left/cee22275_nohash_1.wav'\n",
            " 'speech_dataset/left/9a7c1f83_nohash_2.wav'\n",
            " 'speech_dataset/dog/01d22d03_nohash_1.wav'\n",
            " 'speech_dataset/off/332d33b1_nohash_2.wav']\n",
            "**************************************************\n",
            "Sample of evaluation labels:\n",
            "[1 3 3 0 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUHj8Q1vz2w2",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing<a id='preprocessing'></a>\n",
        "\n",
        "The example model requires some input preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz-AxyySz6ti",
        "colab_type": "text"
      },
      "source": [
        "## WAVE contents\n",
        "\n",
        "The first step is to read the raw binary contents of the input files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJcRJFwnz4Fh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9e5262ee-55fc-4884-e053-d5acf2fa9d47"
      },
      "source": [
        "wav_contents = tf.read_file(os.path.join(data_dir, 'happy/0227998e_nohash_1.wav'))\n",
        "with tf.Session() as sess:\n",
        "    wav_contents_val = sess.run(wav_contents)\n",
        "\n",
        "print('This is how the contents of a WAVE file look like:')\n",
        "wav_contents_val[:100] + ' ... + {} more bytes'.format(len(wav_contents_val[100:])).encode()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is how the contents of a WAVE file look like:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'RIFFpn\\x00\\x00WAVEfmt \\x10\\x00\\x00\\x00\\x01\\x00\\x01\\x00\\x80>\\x00\\x00\\x00}\\x00\\x00\\x02\\x00\\x10\\x00dataLn\\x00\\x00\\xf9\\xff\\xf5\\xff\\xf5\\xff\\xf5\\xff\\xfe\\xff#\\x00p\\x00\\xc8\\x00\\xfa\\x00\\xcb\\x00M\\x00\\xf2\\xff\\xd7\\xff\\xd2\\xff\\xd4\\xff\\xce\\xff\\xe1\\xff\\xf6\\xff\\x0b\\x001\\x00O\\x00g\\x00~\\x00\\xa8\\x00\\xa2\\x00M\\x00\\xc6\\xff=\\xff ... + 28180 more bytes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmOL3KCo0Mra",
        "colab_type": "text"
      },
      "source": [
        "## Decodes WAVE contents to a float32 tensor\n",
        "\n",
        "A decoded WAVE file is a sequence of numbers representing its amplitude values recorded on equal intervals over time.\n",
        "\n",
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.6/kfp-components/notebooks/speech_recognition/assets/audio.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcoAgryl0LjL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17cab58e-75eb-443a-9dc5-fd494a13e783"
      },
      "source": [
        "def decode_wav(wav_contents, desired_samples=16000):\n",
        "    audio = audio_ops.decode_wav(wav_contents, desired_channels=1, desired_samples=desired_samples)[0]\n",
        "    return tf.reshape(audio, [1, -1])\n",
        "\n",
        "print('Function decode_wav returns:', decode_wav(tf.constant(b\"foo\")))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function decode_wav returns: Tensor(\"Reshape:0\", shape=(1, 16000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOxUbsrq0WPR",
        "colab_type": "text"
      },
      "source": [
        "## Audio augmentation\n",
        "\n",
        "The goal here is to randomly modify the input audio in a way that mimics the natural variation of voice. There is much more that can be done as adding background noise and random speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoXFOQXW0Ymr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb9fe187-1b5f-490a-a92a-84b8be46c11f"
      },
      "source": [
        "def audio_augmentation(audio):\n",
        "    # this input is expected to have shape: [1, 16000]\n",
        "    audio = tf.reshape(audio, [16000], name='augmentation_input_reshape')\n",
        "    # random volume addjustment\n",
        "    audio = audio * tf.random.truncated_normal(shape=[], mean=1., stddev=.2)\n",
        "    # random trim from the start and end of the audio\n",
        "    start_offset = tf.random.uniform(shape=[], minval=0, maxval=16000//4, dtype=tf.int32)\n",
        "    end_offset = tf.random.uniform(shape=[], minval=1, maxval=16000//4, dtype=tf.int32)\n",
        "    # negative indexing e.g.: audio[:-end_offset] in TF doesn't work as expected with 0\n",
        "    # so append a dummy 0 at the end have end_offset to be at least 1\n",
        "    audio = tf.concat([audio, [0.]], axis=0)\n",
        "    audio = audio[start_offset:-end_offset]\n",
        "    # change the center of the audio\n",
        "    max_move = start_offset + end_offset\n",
        "    start_pad = tf.random.uniform(shape=[], minval=0, maxval=max_move, dtype=tf.int32)\n",
        "    end_pad = max_move - start_pad - 1\n",
        "    audio = tf.concat([tf.zeros(start_pad), audio, tf.zeros(end_pad)], axis=0)\n",
        "\n",
        "    return tf.reshape(audio, [1, 16000], name='augmentation_output_reshape')\n",
        "\n",
        "print('Function augmentation returns:', audio_augmentation(tf.zeros(shape=(1, 16000), dtype=tf.float32)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function augmentation returns: Tensor(\"augmentation_output_reshape:0\", shape=(1, 16000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr1OAMai0b56",
        "colab_type": "text"
      },
      "source": [
        "### Create spectrogram\n",
        "\n",
        "A [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) is the decoded sequence of amplitude values represented visually as an image. In other words, we convert every WAVE file to a one channel image. This allows us to treats this problem as grayscale image classification.\n",
        "\n",
        "The function `create_spectrogram` creates a spectrogram and changes the resolution and frequency range of the spectrogram to optimize for human speech.\n",
        "\n",
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.6/kfp-components/notebooks/speech_recognition/assets/spectrogram.png)\n",
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.6/kfp-components/notebooks/speech_recognition/assets/audio_s.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pshfIWz0d3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac620ce4-22e0-4c54-a91b-8e0551443efa"
      },
      "source": [
        "def create_spectrogram(audio):\n",
        "    window_size = int(sample_rate * window_size_ms / 1000)\n",
        "    stride = int(sample_rate * window_stride_ms / 1000)\n",
        "    dct_coefficient_count = 40\n",
        "\n",
        "    spectrogram = audio_ops.audio_spectrogram(\n",
        "        input=tf.reshape(audio, [-1, 1]),\n",
        "        window_size=window_size,\n",
        "        stride=stride,\n",
        "        magnitude_squared=True)\n",
        "\n",
        "    speech_optimized_spectrogram = audio_ops.mfcc(\n",
        "        spectrogram,\n",
        "        sample_rate=sample_rate,\n",
        "        dct_coefficient_count=dct_coefficient_count)\n",
        "    \n",
        "    return tf.expand_dims(tf.squeeze(speech_optimized_spectrogram, axis=0), axis=-1)\n",
        "\n",
        "print('Function create_spectrogram returns:', create_spectrogram(tf.zeros(shape=(1, 16000), dtype=tf.float32)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function create_spectrogram returns: Tensor(\"ExpandDims:0\", shape=(98, 40, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvoNtSnC0iO7",
        "colab_type": "text"
      },
      "source": [
        "## Reading input data\n",
        "\n",
        "The input function takes a list of filenames and creates the following transformations:\n",
        "- optional suffle the filenames\n",
        "- read a WAVE file contents into a raw sequence of bytes\n",
        "- decode the file contents to a numerical sequence (amplitude measurments)\n",
        "- create a spectrogram (98x40x1 gray scale image) from the audio\n",
        "- group a number of spectrograms into a batch (Nx98x40x1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VMzFBQL0jSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ed3e1a3-80a5-4dbb-8ce4-ae1b0fe6a302"
      },
      "source": [
        "def input_fn(filenames, labels=None, batch_size=64, repeat=1, shuffle=False, augmentation=False):\n",
        "    if labels is None:\n",
        "        labels = np.zeros(len(filenames))\n",
        "    if not shuffle:\n",
        "        num_parallel_calls = None\n",
        "    else:\n",
        "        num_parallel_calls = 4\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "    \n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=200)\n",
        "\n",
        "    # read file contents\n",
        "    dataset = dataset.map(\n",
        "        lambda filename, label: (tf.read_file(filename), label),\n",
        "        num_parallel_calls=num_parallel_calls)\n",
        "\n",
        "    # decode wav files\n",
        "    dataset = dataset.map(\n",
        "        lambda wav_contents, label: (decode_wav(wav_contents), label),\n",
        "        num_parallel_calls=num_parallel_calls)\n",
        "\n",
        "    # augmentation of the audio\n",
        "    if augmentation:\n",
        "        dataset = dataset.map(\n",
        "            lambda audio, label: (audio_augmentation(audio), label),\n",
        "            num_parallel_calls=num_parallel_calls)\n",
        "\n",
        "    # spectrogram\n",
        "    dataset = dataset.map(\n",
        "        lambda audio, label: ({'spectrogram': create_spectrogram(audio)}, label),\n",
        "        num_parallel_calls=num_parallel_calls)\n",
        "\n",
        "    return dataset.batch(batch_size).repeat(repeat)\n",
        "\n",
        "# showing dataset output\n",
        "input_fn(train_inputs, train_labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ({spectrogram: (?, 98, 40, 1)}, (?,)), types: ({spectrogram: tf.float32}, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj2TC92P0oSR",
        "colab_type": "text"
      },
      "source": [
        "## Model<a id='model'></a>\n",
        "\n",
        "The model created based on the research paper: [Convolutional Neural Networks for Small-footprint Keyword Spotting](http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf). In essence, it chains `Conv -> MaxPool -> Conv -> Dense` layers. This architecture been shown to outperform DNNs with far fewer parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdSc8eA00qD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "8158cb95-1d6d-4205-90e2-55eebeed9a34"
      },
      "source": [
        "def model(spectrograms, is_training):\n",
        "    \n",
        "    net = tf.layers.conv2d(spectrograms, filters=64, kernel_size=(20, 8), padding='same', activation=tf.nn.relu)\n",
        "    net = tf.layers.max_pooling2d(net, pool_size=2, strides=2, padding='same')\n",
        "    net = tf.layers.conv2d(net, filters=64, kernel_size=(10, 4), padding='same', activation=tf.nn.relu)\n",
        "    net = tf.layers.flatten(net)\n",
        "    logits = tf.layers.dense(net, len(vocabulary), activation=None)\n",
        "\n",
        "    return logits\n",
        "\n",
        "print('Function model returns:', model(tf.zeros(shape=[1, 98, 40, 1]), True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-de5b0d15853f>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc19d6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc19d6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc19d6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc19d6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-11-de5b0d15853f>:4: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fc1789e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fc1789e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc1789e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fc1789e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-11-de5b0d15853f>:6: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc1789e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc1789e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From <ipython-input-11-de5b0d15853f>:7: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc1789e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc1789e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc1789e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Function model returns: Tensor(\"dense/BiasAdd:0\", shape=(1, 9), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9frgWQ1E0uE6",
        "colab_type": "text"
      },
      "source": [
        "## Speech classifier using the Estimator API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgtHTCOM0wK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    \n",
        "    logits = model(\n",
        "        spectrograms=features['spectrogram'],\n",
        "        is_training=mode==tf.estimator.ModeKeys.TRAIN)\n",
        "    \n",
        "    predictions = tf.argmax(logits, 1)\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        predictions = {\n",
        "            'class_probability': tf.reshape(tf.nn.softmax(logits), [-1, len(vocabulary)]),\n",
        "            'class_id': tf.reshape(predictions, [-1, 1]),\n",
        "        }\n",
        "        print(predictions)\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
        "        metrics = {'accuracy': accuracy}\n",
        "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate=params['learning_rate'])\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    model_dir=model_dir,\n",
        "    params = {'learning_rate': learning_rate},\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mCR2QhH0y6D",
        "colab_type": "text"
      },
      "source": [
        "# Training<a id='training'></a>\n",
        "\n",
        "The training is not practical without a GPU. Training for 300000 steps takes about 4 hours.\n",
        "\n",
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.6/kfp-components/notebooks/speech_recognition/assets/tensorboard.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi3vRWgf00Rc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b0d3ca6-6756-434f-827c-73ac91b9aa92"
      },
      "source": [
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn=lambda: input_fn(\n",
        "        train_inputs,\n",
        "        train_labels,\n",
        "        batch_size=batch_size,\n",
        "        repeat=-1,\n",
        "        shuffle=True,\n",
        "        augmentation=True),\n",
        "    max_steps=max_training_steps)\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "    input_fn=lambda: input_fn(\n",
        "        eval_inputs,\n",
        "        eval_labels,\n",
        "        batch_size=batch_size,\n",
        "        repeat=1,\n",
        "        shuffle=False,\n",
        "        augmentation=False)\n",
        ")\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fb84acf8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fb84acf8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fb84acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fb84acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62facfa710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62facfa710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62facfa710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62facfa710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a07ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a07ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a07ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a07ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a2fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a2fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a2fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1a2fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1a2fbe0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1a2fbe0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1a2fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1a2fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1a2fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f16d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f16d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f16d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f16d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f16d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62faddeb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62faddeb00>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62faddeb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62faddeb00>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62faddeb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62faddeb00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62faddeb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62faddeb00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1c65a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1c65a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1c65a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1c65a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a07438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a07438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a07438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1a07438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fba09240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fba09240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fba09240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fba09240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fba09240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fba09240>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fba09240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fba09240>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fba09240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fba09240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fba09240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fba09240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f19c77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f19c77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f19c77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f19c77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f190b8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f190b8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f190b8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f190b8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f190b8d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f190b8d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f190b8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f190b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f190b8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1556390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f1556390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1556390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f1556390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f153be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f153be10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f153be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f153be10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f153be10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc14d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc14d160>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc14d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62fc14d160>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc14d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc14d160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc14d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62fc14d160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fade5b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fade5b00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fade5b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62fade5b00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f5229be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f62f5229be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f5229be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f62f5229be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f5229be0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f62f5229be0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f5229be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f5229be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f62f5229be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXKMX6VM05aE",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-bLzXd07ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.evaluate(\n",
        "    input_fn=lambda: input_fn(\n",
        "        eval_inputs, eval_labels, batch_size=batch_size, repeat=1, shuffle=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qojVMDuN0-g2",
        "colab_type": "text"
      },
      "source": [
        "## Single-word input prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdoRxmMh0_h_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_iterator = estimator.predict(\n",
        "    input_fn=lambda: input_fn(eval_inputs),\n",
        "    yield_single_examples=False,\n",
        ")\n",
        "\n",
        "predicted_labels = []\n",
        "\n",
        "for pred in predict_iterator:\n",
        "    predicted_labels += pred['class_id'].flatten().tolist()\n",
        "\n",
        "print('Predicted labels:', predicted_labels[:10], 'True labels:', eval_labels[:10].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu39oFQf1ERS",
        "colab_type": "text"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FnZ4-ko1FdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(confusion_matrix(eval_labels, predicted_labels), columns=vocabulary, index=vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN765Vjm1Ku9",
        "colab_type": "text"
      },
      "source": [
        "# Predictions from streaming audio<a id='serving'></a>\n",
        "\n",
        "We have a model that can predict a single label for fixed length audio. To assign multiple labels to a stream of audio, we can slice the input signal to overlapping windows, make predictions on every window with this model, and aggregate the overlapping predictions. This implementation trains the model with single word audio clips and applies the moving window technique to enable prediction on streaming audio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1PlcFrJ1SgL",
        "colab_type": "text"
      },
      "source": [
        "### Make combined audio file\n",
        "\n",
        "This file will be used to simulate streaming audio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQsO8k3L1Lyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.io import wavfile\n",
        "\n",
        "audios = []\n",
        "for filename in eval_inputs[:10]:\n",
        "    rate, audio = wavfile.read(filename)\n",
        "    audios.append(audio)\n",
        "\n",
        "audios = np.concatenate(audios)\n",
        "wavfile.write('combined.wav', 16000, audios)\n",
        "\n",
        "print('This file combines:', eval_inputs[:10])\n",
        "print('Labels for combined.wav:', eval_labels[:10])\n",
        "ipd.Audio('combined.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK29xOEL1Oyw",
        "colab_type": "text"
      },
      "source": [
        "#### Sliding window technique\n",
        "\n",
        "The following graph demonstrates how a sliding window is taking overlapping patches from the input audio.\n",
        "\n",
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.6/kfp-components/notebooks/speech_recognition/assets/sliding_window.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AkwD5SY1rrN",
        "colab_type": "text"
      },
      "source": [
        "#### Sliding window function\n",
        "\n",
        "The following function is a vectorized implementation of a sliding window. For the test case, we have input audio with shape=(4*16000, ) and the returned windowed audio will have shape=(13, 16000)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Up71xBn1shs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sliding_window(audio, window_size=16000, step_size=4000):\n",
        "    audio = tf.reshape(audio, [-1])\n",
        "    audio_len = tf.cast(tf.reshape(tf.shape(audio), []), tf.int32)\n",
        "\n",
        "    # make the audio lengh multiple of the step size\n",
        "    pad = tf.floormod(audio_len, step_size)\n",
        "    audio = tf.concat([audio, tf.zeros(pad)], axis=0)\n",
        "    audio_len = tf.cast(tf.reshape(tf.shape(audio), []), tf.int32)\n",
        "\n",
        "    number_windows = 1 + (audio_len - window_size) // step_size\n",
        "    #number_windows = tf.reduce_max([number_windows, 1])\n",
        "\n",
        "    padding =  tf.zeros(tf.reduce_max([window_size - audio_len, tf.constant(0)]))\n",
        "    audio = tf.concat([padding, audio], axis=0)\n",
        "\n",
        "    indexes = tf.range(start=0, limit=window_size, delta=1)\n",
        "    row_offsets = tf.range(start=0, limit=number_windows, delta=1)\n",
        "\n",
        "    # broadcasted summation\n",
        "    indexes = indexes[None, :] + (row_offsets[:, None] * step_size)\n",
        "    windowed_audio = tf.gather(audio, indexes, validate_indices=False)\n",
        "    return windowed_audio\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    print(sliding_window(audio=tf.zeros(shape=(4*16000))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NlBPp4Z1xgm",
        "colab_type": "text"
      },
      "source": [
        "### A new input with sliding window\n",
        "\n",
        "Note that this input function takes only one file at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2fljXM41ydg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sliding_window_input_fn(filename):\n",
        "    wav_content = tf.read_file(filename)\n",
        "    audio = decode_wav(wav_content, desired_samples=-1)\n",
        "    windowed_audio = sliding_window(audio)    \n",
        "    spectrograms = tf.map_fn(create_spectrogram, windowed_audio, dtype=tf.float32)\n",
        "    return {'spectrogram': spectrograms}\n",
        "    \n",
        "sliding_window_input_fn('combined.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5smdGsr619rW",
        "colab_type": "text"
      },
      "source": [
        "## Windowed prediction for a multi-word input audio file\n",
        "\n",
        "Note that the raw predictions are created from sliding window with overlapping and "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-LJy-pC1-j9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictino_iterator = estimator.predict(\n",
        "    input_fn=lambda: sliding_window_input_fn('combined.wav'),\n",
        "    yield_single_examples=False,\n",
        ")\n",
        "\n",
        "result = next(predictino_iterator)\n",
        "raw_predictions = result['class_id'].flatten()\n",
        "raw_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9M5Wemr2CVr",
        "colab_type": "text"
      },
      "source": [
        "## Final streaming prediction aggregation\n",
        "\n",
        "To digest this into a final streaming prediction we can use a heuristic rule of taking rolling mode and combining up to file consecutive ids into one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9UhUswT2DJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "\n",
        "def mode_of_non_zeros(values):\n",
        "    other_index = vocabulary.index('__other__')\n",
        "    if np.all(values==other_index):\n",
        "        return other_index\n",
        "    else:\n",
        "        return scipy.stats.mode(window)[0][0]\n",
        "    \n",
        "max_word_distance = 5\n",
        "\n",
        "predictions = []\n",
        "while len(raw_predictions):\n",
        "    window = raw_predictions[:10]\n",
        "    prediction = mode_of_non_zeros(window)\n",
        "    if len(predictions) and prediction == predictions[-1] and distance < max_word_distance:\n",
        "        distance += 1\n",
        "    else:\n",
        "        distance = 1\n",
        "        predictions.append(prediction)\n",
        "    raw_predictions = raw_predictions[1:]\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "print('Streaming audio predictions:', predictions)\n",
        "print('Streaming audio true labels:', eval_labels[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmcV7oSV2HBB",
        "colab_type": "text"
      },
      "source": [
        "## References<a id='references'></a>\n",
        "\n",
        "- [Tensorflow: Simple audio recognition tutorial](https://www.tensorflow.org/tutorials/sequences/audio_recognition)\n",
        "- [Speech_commands dataset v0.02](https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz)\n",
        "- [Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition](https://arxiv.org/abs/1804.03209)\n",
        "- [WAV](https://en.wikipedia.org/wiki/WAV)"
      ]
    }
  ]
}